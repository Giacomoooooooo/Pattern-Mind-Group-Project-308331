{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b726c67b",
   "metadata": {},
   "source": [
    "PatternMind — Visual Space Organization\n",
    "This notebook contains all code for the PatternMind project. Text and code cells strictly alternate as required.\n",
    "\n",
    "How to use:\n",
    "\n",
    "Put your dataset on disk as folder-per-class.\n",
    "Set DATASET_DIR below.\n",
    "Run cells top-to-bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a353a0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "from pathlib import Path\n",
    "\n",
    "# TODO: change this to your dataset root folder (one subfolder per class)\n",
    "DATASET_DIR = Path(\"patternmind_dataset\")\n",
    "\n",
    "# Output folder for figures required by README\n",
    "OUT_DIR = Path(\"images\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81436f9",
   "metadata": {},
   "source": [
    "1) Imports and reproducibility\n",
    "We import libraries and set seeds for reproducible feature extraction, sampling, and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fe2dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "try:\n",
    "    import umap\n",
    "    UMAP_AVAILABLE = True\n",
    "except Exception:\n",
    "    UMAP_AVAILABLE = False\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b7ee91",
   "metadata": {},
   "source": [
    "2) Dataset indexing\n",
    "We scan the dataset directory and build a dataframe with image paths and labels. This lets us run EDA, sampling, and later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15357bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "def index_image_folder(root: Path, exts=(\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\")) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    root = Path(root)\n",
    "    assert root.exists(), f\"DATASET_DIR not found: {root}\"\n",
    "    for class_dir in sorted([p for p in root.iterdir() if p.is_dir()]):\n",
    "        label = class_dir.name\n",
    "        for ext in exts:\n",
    "            for fp in class_dir.rglob(f\"*{ext}\"):\n",
    "                rows.append({\"path\": str(fp), \"label\": label})\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No images found. Check DATASET_DIR and file extensions.\")\n",
    "    return df\n",
    "\n",
    "df = index_image_folder(DATASET_DIR)\n",
    "df.head(), df[\"label\"].nunique(), len(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3a1f56",
   "metadata": {},
   "source": [
    "3) EDA (Exploratory Data Analysis)\n",
    "We inspect class counts and visualize a few example images per category. This checks dataset balance and potential noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b9fa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "class_counts = df[\"label\"].value_counts().sort_values(ascending=False)\n",
    "display(class_counts.head(20))\n",
    "\n",
    "#plt.figure()\n",
    "#class_counts.plot(kind=\"bar\")\n",
    "#plt.title(\"Images per class\")\n",
    "#plt.ylabel(\"count\")\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n",
    "\n",
    "counts = df[\"label\"].value_counts()\n",
    "\n",
    "\n",
    "print(f\"Number of classes: {counts.shape[0]}\")\n",
    "print(f\"Min images per class: {counts.min()}\")\n",
    "print(f\"Max images per class: {counts.max()}\")\n",
    "print(f\"Mean images per class: {counts.mean():.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e236e6",
   "metadata": {},
   "source": [
    "Example grid per class (small sample)\n",
    "We draw a simple preview grid for a subset of classes to ensure loading works and to understand intra-class variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c190b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def show_examples(df: pd.DataFrame, n_classes=6, n_per_class=6, figsize=(12, 8)):\n",
    "    labels = list(df[\"label\"].unique())[:n_classes]\n",
    "    sample = (df[df[\"label\"].isin(labels)]\n",
    "              .groupby(\"label\", group_keys=False)\n",
    "              .apply(lambda x: x.sample(min(n_per_class, len(x)), random_state=SEED)))\n",
    "    rows = len(labels)\n",
    "    cols = n_per_class\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i, (lbl, group) in enumerate(sample.groupby(\"label\")):\n",
    "        for j, (_, r) in enumerate(group.reset_index(drop=True).iterrows()):\n",
    "            ax = plt.subplot(rows, cols, i*cols + j + 1)\n",
    "            img = Image.open(r[\"path\"]).convert(\"RGB\")\n",
    "            ax.imshow(img)\n",
    "            ax.axis(\"off\")\n",
    "            if j == 0:\n",
    "                ax.set_title(lbl, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_examples(df, n_classes=min(6, df[\"label\"].nunique()), n_per_class=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de630f7",
   "metadata": {},
   "source": [
    "4) Feature extraction (Deep CNN)\n",
    "We use a pretrained CNN as a fixed feature extractor. For each image we obtain a vector representation from the penultimate layer. This representation is the basis for dimensionality reduction and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07efbe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing for pretrained models\n",
    "IMG_SIZE = 224\n",
    "tfm = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df.loc[idx, \"path\"]\n",
    "        label = self.df.loc[idx, \"label\"]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label, path\n",
    "\n",
    "# Pretrained backbone\n",
    "backbone = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "# Remove classification head => output features (2048)\n",
    "backbone.fc = nn.Identity()\n",
    "backbone = backbone.to(device).eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_cnn_features(df: pd.DataFrame, batch_size=64, num_workers=2):\n",
    "    ds = ImageDataset(df, transform=tfm)\n",
    "    dl = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=False,\n",
    "                                     num_workers=num_workers, pin_memory=True)\n",
    "    feats, labels, paths = [], [], []\n",
    "    for xb, yb, pb in tqdm(dl, desc=\"Extracting CNN features\"):\n",
    "        xb = xb.to(device)\n",
    "        fb = backbone(xb).detach().cpu().numpy()\n",
    "        feats.append(fb)\n",
    "        labels.extend(list(yb))\n",
    "        paths.extend(list(pb))\n",
    "    X = np.vstack(feats)\n",
    "    return X, np.array(labels), np.array(paths)\n",
    "\n",
    "# Optionally subsample for speed if dataset is huge\n",
    "MAX_IMAGES = 1500\n",
    "if len(df) > MAX_IMAGES:\n",
    "    df_run = df.sample(MAX_IMAGES, random_state=SEED).reset_index(drop=True)\n",
    "else:\n",
    "    df_run = df.copy()\n",
    "\n",
    "X_cnn, y, paths = extract_cnn_features(df_run, batch_size=64, num_workers=2)\n",
    "X = X_cnn\n",
    "X_cnn.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e31a54",
   "metadata": {},
   "source": [
    "5) Baseline features (Color histograms)\n",
    "As a classical baseline, we compute RGB color histograms. This often clusters by color themes (e.g., flags, sky, grass) and provides a useful contrast to CNN features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a96b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_histogram(img: Image.Image, bins=16):\n",
    "    arr = np.asarray(img.convert(\"RGB\"))\n",
    "    feat = []\n",
    "    for c in range(3):\n",
    "        h, _ = np.histogram(arr[:,:,c], bins=bins, range=(0,255), density=True)\n",
    "        feat.append(h)\n",
    "    return np.concatenate(feat).astype(np.float32)\n",
    "\n",
    "def extract_color_hist_features(df: pd.DataFrame, bins=16):\n",
    "    feats = []\n",
    "    for p in tqdm(df[\"path\"].tolist(), desc=\"Extracting color hist features\"):\n",
    "        img = Image.open(p).convert(\"RGB\").resize((IMG_SIZE, IMG_SIZE))\n",
    "        feats.append(color_histogram(img, bins=bins))\n",
    "    return np.vstack(feats)\n",
    "\n",
    "X_col = extract_color_hist_features(df_run, bins=16)\n",
    "X_col.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6bfbc6",
   "metadata": {},
   "source": [
    "## 6) Standardization\n",
    "We standardize features before PCA/UMAP and clustering. This improves stability for distance-based methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cceb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_cnn = StandardScaler()\n",
    "Xc = scaler_cnn.fit_transform(X_cnn)\n",
    "\n",
    "scaler_col = StandardScaler()\n",
    "Xh = scaler_col.fit_transform(X_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172b5d65",
   "metadata": {},
   "source": [
    "## 7) Dimensionality reduction (PCA / t-SNE / UMAP)\n",
    "We compute low-dimensional embeddings for visualization. We will generate and save figures for the README in the `images/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a54508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_pca(X, n=2):\n",
    "    return PCA(n_components=n, random_state=SEED).fit_transform(X)\n",
    "\n",
    "def embed_tsne(X, n=2, perplexity=30):\n",
    "    return TSNE(n_components=n, perplexity=perplexity, init=\"pca\",\n",
    "                learning_rate=\"auto\", random_state=SEED).fit_transform(X)\n",
    "\n",
    "def embed_umap(X, n=2, n_neighbors=15, min_dist=0.1):\n",
    "    if not UMAP_AVAILABLE:\n",
    "        raise ImportError(\"umap-learn not installed. Install it or skip UMAP.\")\n",
    "    return umap.UMAP(n_components=n, n_neighbors=n_neighbors, min_dist=min_dist,\n",
    "                     random_state=SEED).fit_transform(X)\n",
    "\n",
    "def plot_embedding(Z, labels, title, outpath=None, max_points=5000):\n",
    "    # If many points, subsample for readability\n",
    "    if len(Z) > max_points:\n",
    "        idx = np.random.RandomState(SEED).choice(len(Z), size=max_points, replace=False)\n",
    "        Zp = Z[idx]\n",
    "        lp = labels[idx]\n",
    "    else:\n",
    "        Zp, lp = Z, labels\n",
    "\n",
    "    plt.figure()\n",
    "    # Map labels to integers for scatter (matplotlib handles categories poorly)\n",
    "    lab_to_i = {lab:i for i, lab in enumerate(sorted(set(lp)))}\n",
    "    c = np.array([lab_to_i[a] for a in lp])\n",
    "    plt.scatter(Zp[:,0], Zp[:,1], c=c, s=8)\n",
    "    plt.title(title)\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    if outpath:\n",
    "        plt.savefig(outpath, dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "# CNN feature embeddings\n",
    "Z_pca = embed_pca(Xc, 2)\n",
    "plot_embedding(Z_pca, y, \"PCA (CNN features)\", OUT_DIR/\"embedding_pca.png\")\n",
    "\n",
    "Z_tsne = embed_tsne(Xc, 2, perplexity=30)\n",
    "plot_embedding(Z_tsne, y, \"t-SNE (CNN features)\", OUT_DIR/\"embedding_tsne.png\")\n",
    "\n",
    "if UMAP_AVAILABLE:\n",
    "    Z_umap = embed_umap(Xc, 2, n_neighbors=15, min_dist=0.1)\n",
    "    plot_embedding(Z_umap, y, \"UMAP (CNN features)\", OUT_DIR/\"embedding_umap.png\")\n",
    "else:\n",
    "    Z_umap = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae32cb7a",
   "metadata": {},
   "source": [
    "## 8) Clustering\n",
    "We apply several clustering algorithms to **organize** the visual space. We evaluate with internal metrics (Silhouette, Davies–Bouldin) and, for interpretation only, external agreement with labels (ARI/NMI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bbc05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_clustering(X, cluster_ids, labels_true=None):\n",
    "    # Internal metrics (no labels needed)\n",
    "    uniq = set(cluster_ids)\n",
    "    sil = silhouette_score(X, cluster_ids) if len(uniq) > 1 else np.nan\n",
    "    dbi = davies_bouldin_score(X, cluster_ids) if len(uniq) > 1 else np.nan\n",
    "\n",
    "    out = {\"n_clusters\": len(uniq), \"silhouette\": sil, \"davies_bouldin\": dbi}\n",
    "\n",
    "    # External metrics (only if labels are provided)\n",
    "    if labels_true is not None:\n",
    "        out[\"ARI\"] = adjusted_rand_score(labels_true, cluster_ids)\n",
    "        out[\"NMI\"] = normalized_mutual_info_score(labels_true, cluster_ids)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# More realistic K values: coarse visual groupings (not classes)\n",
    "# ------------------------------------------------------------\n",
    "K_LIST = [10, 15, 20, 30]\n",
    "\n",
    "for k in K_LIST:\n",
    "    # KMeans\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10, random_state=SEED)\n",
    "    cid_km = kmeans.fit_predict(Xc)\n",
    "    results.append((f\"KMeans(k={k})\", eval_clustering(Xc, cid_km, y)))\n",
    "\n",
    "    # Agglomerative\n",
    "    agg = AgglomerativeClustering(n_clusters=k)\n",
    "    cid_ag = agg.fit_predict(Xc)\n",
    "    results.append((f\"Agglomerative(k={k})\", eval_clustering(Xc, cid_ag, y)))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# DBSCAN: show that it often collapses without careful tuning\n",
    "# ------------------------------------------------------------\n",
    "for eps in [1.5, 2.0, 2.5, 3.0]:\n",
    "    db = DBSCAN(eps=eps, min_samples=10)\n",
    "    cid_db = db.fit_predict(Xc)\n",
    "    results.append((f\"DBSCAN(eps={eps})\", eval_clustering(Xc, cid_db, y)))\n",
    "\n",
    "pd.DataFrame([{**v, \"method\": m} for m, v in results]).set_index(\"method\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943e39ec",
   "metadata": {},
   "source": [
    "### Cluster visualization\n",
    "We visualize cluster assignments on a 2D embedding (UMAP if available, else PCA). This is key for interpretability: compact clusters, overlaps, and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9614f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CLUSTERING (fit in feature space X, visualize in 2D)\n",
    "# ============================================================\n",
    "\n",
    "def plot_clusters_on_embedding(\n",
    "    Z, cluster_ids, title, outpath=None, max_points=5000, alpha=0.7\n",
    "):\n",
    "    if len(Z) > max_points:\n",
    "        idx = np.random.RandomState(SEED).choice(len(Z), size=max_points, replace=False)\n",
    "        Zp = Z[idx]\n",
    "        cp = np.array(cluster_ids)[idx]\n",
    "    else:\n",
    "        Zp = Z\n",
    "        cp = np.array(cluster_ids)\n",
    "\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.scatter(Zp[:, 0], Zp[:, 1], c=cp, s=8, alpha=alpha)\n",
    "    plt.title(title)\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    if outpath:\n",
    "        plt.savefig(outpath, dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Choose visualization embedding (ONLY for plotting)\n",
    "# ------------------------------------------------------------\n",
    "Z_vis = Z_umap if Z_umap is not None else Z_pca\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# KMEANS (on 2048D features)\n",
    "# ------------------------------------------------------------\n",
    "k = 15  # intentionally small: visual archetypes, not classes\n",
    "kmeans = KMeans(n_clusters=k, random_state=SEED, n_init=10)\n",
    "cid_km = kmeans.fit_predict(X)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# AGGLOMERATIVE (on 2048D features)\n",
    "# ------------------------------------------------------------\n",
    "agg = AgglomerativeClustering(n_clusters=k)\n",
    "cid_ag = agg.fit_predict(X)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# DBSCAN (on 2048D features)\n",
    "# ------------------------------------------------------------\n",
    "db = DBSCAN(eps=3.0, min_samples=10)\n",
    "cid_db = db.fit_predict(X)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# VISUALIZE CLUSTERS (computed in X, shown in Z)\n",
    "# ------------------------------------------------------------\n",
    "plot_clusters_on_embedding(\n",
    "    Z_vis,\n",
    "    cid_km,\n",
    "    \"KMeans clusters (fit in 2048D, shown in 2D)\",\n",
    "    OUT_DIR / \"clusters_kmeans.png\"\n",
    ")\n",
    "\n",
    "plot_clusters_on_embedding(\n",
    "    Z_vis,\n",
    "    cid_ag,\n",
    "    \"Agglomerative clusters (fit in 2048D, shown in 2D)\",\n",
    "    OUT_DIR / \"clusters_agglomerative.png\"\n",
    ")\n",
    "\n",
    "plot_clusters_on_embedding(\n",
    "    Z_vis,\n",
    "    cid_db,\n",
    "    \"DBSCAN clusters (fit in 2048D, shown in 2D)\",\n",
    "    OUT_DIR / \"clusters_dbscan.png\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d2a7b",
   "metadata": {},
   "source": [
    "## 9) Cluster–Label relationship (ambiguity / purity)\n",
    "Although labels are not used to train the clustering, they help **interpret** results. We compute a cluster×label table to find:\n",
    "- clusters mixing multiple labels (ambiguity)\n",
    "- labels split across many clusters (high intra-class variability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507e4dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_label_table(cluster_ids, labels):\n",
    "    tab = pd.crosstab(pd.Series(cluster_ids, name=\"cluster\"),\n",
    "                      pd.Series(labels, name=\"label\"))\n",
    "    return tab\n",
    "\n",
    "tab_km = cluster_label_table(cid_km, y)\n",
    "tab_km.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06efdfa",
   "metadata": {},
   "source": [
    "Heatmap-like visualization (simple)\n",
    "We save a figure that communicates which labels dominate each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cb230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_table_heatmap_like(tab: pd.DataFrame, title: str, outpath: Path, top_labels=20):\n",
    "    # Optionally reduce columns for readability\n",
    "    if tab.shape[1] > top_labels:\n",
    "        cols = tab.sum(axis=0).sort_values(ascending=False).head(top_labels).index\n",
    "        tab = tab[cols]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(tab.values, aspect=\"auto\")\n",
    "    plt.title(title)\n",
    "    plt.yticks(range(tab.shape[0]), tab.index)\n",
    "    plt.xticks(range(tab.shape[1]), tab.columns, rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath, dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "save_table_heatmap_like(tab_km, \"Cluster × Label counts (KMeans)\", OUT_DIR/\"cluster_label_heatmap.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf9a6c0",
   "metadata": {},
   "source": [
    "10) Nearest-neighbor analysis (semantic relationships)\n",
    "To uncover relationships and ambiguities, we compute nearest neighbors in feature space. We then inspect cases where nearest neighbors belong to different labels, indicating visual similarity across categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d948e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CLEAN SUMMARY (replaces the \"Time series / Values / Faceted distributions\" mess)\n",
    "# ============================================================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# --- Dataset summary (readable) ---\n",
    "class_counts = df_run[\"label\"].value_counts()\n",
    "print(f\"Images: {len(df_run)}\")\n",
    "print(f\"Classes: {class_counts.shape[0]}\")\n",
    "print(f\"Min / Mean / Max images per class: {class_counts.min()} / {class_counts.mean():.2f} / {class_counts.max()}\")\n",
    "\n",
    "display(class_counts.head(15).rename(\"count\").to_frame())  # top-15 classes only\n",
    "\n",
    "# Optional: simple histogram of class sizes (much more readable than 230 bars)\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(class_counts.values, bins=30)\n",
    "plt.title(\"Distribution of images per class\")\n",
    "plt.xlabel(\"Images per class\")\n",
    "plt.ylabel(\"Number of classes\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Cluster purity summary (compact + meaningful) ---\n",
    "def cluster_purity(cluster_ids, labels_true):\n",
    "    tab = pd.crosstab(pd.Series(cluster_ids, name=\"cluster\"), pd.Series(labels_true, name=\"label\"))\n",
    "    purity = (tab.max(axis=1) / tab.sum(axis=1)).sort_values(ascending=False)\n",
    "    return purity\n",
    "\n",
    "def show_purity(cluster_ids, name):\n",
    "    purity = cluster_purity(cluster_ids, y)\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  clusters: {purity.shape[0]}\")\n",
    "    print(f\"  mean purity: {purity.mean():.3f}\")\n",
    "    print(f\"  median purity: {purity.median():.3f}\")\n",
    "    display(purity.head(10).rename(\"purity\").to_frame())  # top 10 purest clusters\n",
    "\n",
    "# These variables should already exist from your clustering step:\n",
    "# cid_km, cid_ag, cid_db\n",
    "show_purity(cid_km, \"KMeans purity\")\n",
    "show_purity(cid_ag, \"Agglomerative purity\")\n",
    "# DBSCAN might produce -1 noise; purity still works, but often not very informative:\n",
    "show_purity(cid_db, \"DBSCAN purity\")\n",
    "\n",
    "# --- Nearest-neighbor cross-label rate (strong evidence) ---\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=6, metric=\"euclidean\")\n",
    "nn.fit(Xc)\n",
    "dists, idxs = nn.kneighbors(Xc)\n",
    "\n",
    "rows = []\n",
    "for i in range(len(Xc)):\n",
    "    for jpos in range(1, 6):  # skip self\n",
    "        j = idxs[i, jpos]\n",
    "        rows.append({\n",
    "            \"label_i\": y[i],\n",
    "            \"label_j\": y[j],\n",
    "            \"distance\": float(dists[i, jpos]),\n",
    "            \"same_label\": (y[i] == y[j]),\n",
    "            \"path_i\": paths[i],\n",
    "            \"path_j\": paths[j],\n",
    "        })\n",
    "\n",
    "nn_df = pd.DataFrame(rows)\n",
    "cross_label_rate = (~nn_df[\"same_label\"]).mean()\n",
    "print(f\"\\nNearest-neighbor cross-label rate (k=5): {cross_label_rate:.3f}\")\n",
    "\n",
    "print(\"\\nClosest cross-label neighbor pairs (top 10):\")\n",
    "display(nn_df[~nn_df[\"same_label\"]].sort_values(\"distance\").head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d131eb2f",
   "metadata": {},
   "source": [
    "Save a small nearest-neighbor gallery figure\n",
    "We create a compact gallery for a few query images with their nearest neighbors. This directly supports the narrative about ambiguity and category overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f071a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nn_gallery(sample_indices, idxs, paths, labels, outpath: Path, n_neighbors=6, thumb_size=128):\n",
    "    cols = n_neighbors\n",
    "    rows = len(sample_indices)\n",
    "    fig_w = cols * 2\n",
    "    fig_h = rows * 2\n",
    "    plt.figure(figsize=(fig_w, fig_h))\n",
    "    for r, i in enumerate(sample_indices):\n",
    "        neigh = idxs[i, :n_neighbors]\n",
    "        for c, j in enumerate(neigh):\n",
    "            ax = plt.subplot(rows, cols, r*cols + c + 1)\n",
    "            img = Image.open(paths[j]).convert(\"RGB\").resize((thumb_size, thumb_size))\n",
    "            ax.imshow(img)\n",
    "            ax.axis(\"off\")\n",
    "            if c == 0:\n",
    "                ax.set_title(f\"Q: {labels[j]}\", fontsize=9)\n",
    "            else:\n",
    "                ax.set_title(labels[j], fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath, dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "# pick a few diverse queries\n",
    "rng = np.random.RandomState(SEED)\n",
    "sample_indices = rng.choice(len(Xc), size=min(6, len(Xc)), replace=False)\n",
    "make_nn_gallery(sample_indices, idxs, paths, y, OUT_DIR/\"nn_gallery.png\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
